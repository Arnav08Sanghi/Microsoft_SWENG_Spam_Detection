{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.TextPreprocessingUtils import vectorize_text_data\n",
    "train, test = vectorize_text_data(\"data/raw_data.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(test.shape[1]):\n",
    "    tfidf_score = test.iat[5,x] # For the every column (word in vocabulary) in first tweet (row 0)\n",
    "    if (tfidf_score > 0.0):\n",
    "        label = test.columns[x]\n",
    "        print(\"TF-IDF Score for word \" + label + \" is: \" + str(tfidf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "had to download the below for textpreprocessingutils.py to work, thought it might be useful for others - joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing text data...\n",
      "Vectorizing text data with TF-IDF...\n",
      "TF-IDF Vectorizer saved as 'tfidf_vectorizer.pkl'.\n",
      "Splitting data into training and testing sets...\n",
      "TF-IDF vectorization completed, and data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from backend.TextPreprocessingUtils import vectorize_text_data\n",
    "\n",
    "df_train, df_test, label_train, label_test = vectorize_text_data('data/raw_data.csv')\n",
    "\n",
    "# df_list = []\n",
    "\n",
    "# for chunk in chunks:\n",
    "#     Identify columns that are float64\n",
    "#     float_cols = chunk.select_dtypes(include=['float64']).columns\n",
    "     # Convert those columns to float32\n",
    "#     chunk[float_cols] = chunk[float_cols].astype(\"float32\")\n",
    "#     df_list.append(chunk)\n",
    "\n",
    "# df = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pop out the column with value \"label\"\n",
    "label_column = df.pop(\"label\")\n",
    "\n",
    "# Insert it back at index 0\n",
    "df.insert(0, \"label\", label_column)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Score for word http is: 1.0\n"
     ]
    }
   ],
   "source": [
    "for x in range(df_train.shape[1]):\n",
    "    tfidf_score = df_train.iat[5,x] # For the every column (word in vocabulary) in first tweet (row 0)\n",
    "    if (tfidf_score > 0.0):\n",
    "        label = df_train.columns[x]\n",
    "        print(\"TF-IDF Score for word \" + label + \" is: \" + str(tfidf_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576000, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = df.drop('label', axis=1).values\n",
    "\n",
    "# Normalize the features so they have a mean of 0 and std of 1\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to be [samples, timesteps, features]\n",
    "xtrain = df_train.to_numpy()\n",
    "xtest = df_test.to_numpy()\n",
    "ytrain = label_train.to_numpy()\n",
    "ytest = label_test.to_numpy()\n",
    "\n",
    "# xtrain = xtrain.reshape((xtrain.shape[0], xtrain.shape[1], 1))\n",
    "# xtest = xtest.reshape((xtest.shape[0], xtest.shape[1], 1))\n",
    "\n",
    "\n",
    "\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatted X and y data\n"
     ]
    }
   ],
   "source": [
    "# Separate labels and features\n",
    "# y = df['label'].values\n",
    "# X = df.drop('label', axis=1).values\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "xtrain_reduced = svd.fit_transform(xtrain)\n",
    "xtest_reduced = svd.transform(xtest)\n",
    "\n",
    "joblib.dump(svd, 'svd.pkl')\n",
    "training_amount = int(len(xtrain) * 0.01)\n",
    "\n",
    "testing_x = xtest_reduced[-training_amount:]\n",
    "testing_y = ytest[-training_amount:]\n",
    "\n",
    "print(\"formatted X and y data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed class weights: {0: np.float64(0.5996951549628938), 1: np.float64(3.0076444295918794)}\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights to help counter class imbalance\n",
    "classes = np.unique(ytrain)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=ytrain)\n",
    "class_weight_dict = {int(cls): weight for cls, weight in zip(classes, class_weights)}\n",
    "print(\"Computed class weights:\", class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrjoe\\OneDrive\\Desktop\\New folder\\sweng25_group11-microsoftspamdetection\\sklearn-env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 159ms/step - accuracy: 0.6795 - loss: 0.5935\n",
      "Epoch 2/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 169ms/step - accuracy: 0.7140 - loss: 0.5709\n",
      "Epoch 3/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 177ms/step - accuracy: 0.7124 - loss: 0.5696\n",
      "Epoch 4/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 157ms/step - accuracy: 0.7120 - loss: 0.5697\n",
      "Epoch 5/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 488ms/step - accuracy: 0.7154 - loss: 0.5688\n",
      "Epoch 6/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 130ms/step - accuracy: 0.7149 - loss: 0.5687\n",
      "Epoch 7/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 155ms/step - accuracy: 0.7161 - loss: 0.5682\n",
      "Epoch 8/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 173ms/step - accuracy: 0.7157 - loss: 0.5672\n",
      "Epoch 9/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 154ms/step - accuracy: 0.7156 - loss: 0.5685\n",
      "Epoch 10/10\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 145ms/step - accuracy: 0.7158 - loss: 0.5664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21f3dee4560>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(xtrain.shape[1], 1)),  # First layer\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=(xtrain.shape[1], 1)),  # First layer\n",
    "    # Conv1D(filters=32, kernel_size=5, activation='relu'),  # Second layer\n",
    "    # Conv1D(filters=32, kernel_size=5, activation='relu'),  # Third layer\n",
    "    Conv1D(filters=8, kernel_size=5, activation='relu'),   # Fourth layer\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')  # Output: Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(xtrain, ytrain, epochs=10, batch_size=640, class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cnn.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = 0\n",
    "num_tests = 500\n",
    "\n",
    "for i in range(num_tests):\n",
    "    # Choose a sample index from your dataset (you can change this index as needed)\n",
    "    sample_index = np.random.randint(0, testing_x.shape[0])\n",
    "    sample_input = testing_x[sample_index].reshape(1, testing_x.shape[1])  # shape (1, timesteps)\n",
    "\n",
    "    # Get the model's predicted probability for the sample being spam\n",
    "    predicted_probability = model.predict(sample_input, verbose=0)\n",
    "\n",
    "    # Retrieve the actual label from your labels array\n",
    "    actual_label = testing_y[sample_index]\n",
    "\n",
    "    predicted_label = round(predicted_probability)\n",
    "\n",
    "    percentage_success = success_count / (i + 1) * 100\n",
    "    print(f\"{i=} {percentage_success:.2f}% o={predicted_probability:.4f} p={predicted_label} a={actual_label}         \", end=\"\\r\")\n",
    "    success_count += predicted_label == actual_label\n",
    "\n",
    "print(f\"{success_count=} success! (thats like {(success_count / num_tests * 100):.4f}%)    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in the keras format\n",
    "model.save(\"spam_detection_model_CNN.keras\")\n",
    "\n",
    "# Alternatively, to save in the TensorFlow SavedModel format:\n",
    "# model.save(\"spam_detection_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(58)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "success_count = 0\n",
    "for i in range(100):\n",
    "    # Choose a sample index from your dataset (you can change this index as needed)\n",
    "    sample_index = np.random.randint(0, xtest.shape[0])\n",
    "    sample_input = xtest_reduced[sample_index].reshape(1, xtest_reduced.shape[1], 1)  # shape (1, timesteps, features)\n",
    "\n",
    "    # Get the model's predicted probability for the sample being spam\n",
    "    predicted_probability = model.predict(sample_input, verbose=0)[0][0]\n",
    "\n",
    "    # Retrieve the actual label from your labels array\n",
    "    actual_label = y[sample_index]\n",
    "\n",
    "    predicted_label = round(predicted_probability)\n",
    "    success_count += predicted_label == actual_label\n",
    "\n",
    "success_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________\n",
    "The below LSTM didn't work - keeping here for the moment for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Separate labels and features\n",
    "y = df['label'].values\n",
    "X = df.drop('label', axis=1).values\n",
    "\n",
    "# Normalize the features so they have a mean of 0 and std of 1\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to be [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Compute class weights to address class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(\"Computed class weights:\", class_weight_dict)\n",
    "\n",
    "# Define a more complex LSTM model architecture\n",
    "model = Sequential()\n",
    "# First LSTM layer with return_sequences=True to stack another LSTM\n",
    "model.add(LSTM(64, input_shape=(X.shape[1], 1), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second LSTM layer (no return_sequences needed for final output)\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Final dense layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X, y, epochs=10, batch_size=64, validation_split=0.2, class_weight=class_weight_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "svm_model = joblib.load(\"svm.pkl\")\n",
    "print(\"Model loaded successfully with joblib!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
