{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing text data...\n",
      "Vectorizing text data with TF-IDF...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/tfidf_vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTextPreprocessingUtils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vectorize_text_data\n\u001b[1;32m---> 10\u001b[0m xtrain, xtest, ytrain, ytest \u001b[38;5;241m=\u001b[39m \u001b[43mvectorize_text_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/balanced_raw_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\New folder\\sweng25_group11-microsoftspamdetection\\backend\\TextPreprocessingUtils.py:59\u001b[0m, in \u001b[0;36mvectorize_text_data\u001b[1;34m(data_csv_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(\n\u001b[0;32m     52\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,     \u001b[38;5;66;03m# Increased vocab size\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,              \u001b[38;5;66;03m# Include rarer but not super-rare terms\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m,           \u001b[38;5;66;03m# Exclude overly common terms\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)     \u001b[38;5;66;03m# Use unigrams and bigrams\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     57\u001b[0m csr_embed \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_message\u001b[39m\u001b[38;5;124m'\u001b[39m])     \u001b[38;5;66;03m# The vectorizer saves to a csr martix (sparse)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend/tfidf_vectorizer.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF Vectorizer saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_vectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m df_embed \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(csr_embed)     \u001b[38;5;66;03m# Create dataframe from sparse matrix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrjoe\\OneDrive\\Desktop\\New folder\\sweng25_group11-microsoftspamdetection\\sklearn-env\\Lib\\site-packages\\joblib\\numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backend/tfidf_vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from backend.TextPreprocessingUtils import vectorize_text_data\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = vectorize_text_data(\"data/balanced_raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search with LinearSVC...\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Parameters: {'C': 0.1}\n",
      "Best Estimator: LinearSVC(C=0.1, class_weight='balanced', max_iter=10000)\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.67      0.78    119756\n",
      "           1       0.31      0.73      0.44     24244\n",
      "\n",
      "    accuracy                           0.68    144000\n",
      "   macro avg       0.62      0.70      0.61    144000\n",
      "weighted avg       0.82      0.68      0.72    144000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "# Define a fast and simple parameter grid (no gamma or kernel needed)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10]  # I can expand later if needed\n",
    "}\n",
    "\n",
    "# Use LinearSVC: much faster for text data\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=LinearSVC(class_weight='balanced', max_iter=10000),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Time the grid search\n",
    "print(\"Starting Grid Search with LinearSVC...\")\n",
    "\n",
    "grid_search.fit(xtrain.to_numpy(), ytrain.to_numpy())\n",
    "\n",
    "# Output best model and evaluate\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(xtest.to_numpy())\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(ytest, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0.59969515 3.00764443]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(ytrain)\n",
    "print(classes)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=ytrain)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6823541666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_model.score(xtest.to_numpy(), ytest.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91    119756\n",
      "           1       0.69      0.05      0.09     24244\n",
      "\n",
      "    accuracy                           0.84    144000\n",
      "   macro avg       0.76      0.52      0.50    144000\n",
      "weighted avg       0.81      0.84      0.77    144000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize and train the Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_nb = nb.predict(xtest)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(ytest, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(best_model, 'svm.pkl')\n",
    "joblib.dump(nb, 'nb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "svm_model = joblib.load(\"svm.pkl\")\n",
    "#cnn_model = joblib.load(\"cnn.pkl\")\n",
    "# nb_model = joblib.load(\"nb.pkl\")\n",
    "\n",
    "#good_cnn_model = load_model('cnn.keras')\n",
    "\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_pred_svm \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mxtest\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#y_pred_cnn = cnn_model.predict(xtest.to_numpy())\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#y_pred_nb = nb_model.predict(xtest.to_numpy())\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#threshold = 0.6  # Adjust freely\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#y_pred_good_cnn = (y_prob_good_cnn > threshold).astype(int)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xtest' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_svm = svm_model.predict(xtest.to_numpy())\n",
    "#y_pred_cnn = cnn_model.predict(xtest.to_numpy())\n",
    "#y_pred_nb = nb_model.predict(xtest.to_numpy())\n",
    "\n",
    "# Predict probabilities\n",
    "#y_prob_good_cnn = good_cnn_model.predict(xtest.to_numpy())\n",
    "\n",
    "# Apply threshold to convert probabilities to binary class labels\n",
    "#threshold = 0.6  # Adjust freely\n",
    "#y_pred_good_cnn = (y_prob_good_cnn > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn_labels = (y_pred_cnn >= 0.5).astype(int)\n",
    "#y_pred_good_cnn_labels = (y_pred_normal >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrjoe\\OneDrive\\Desktop\\New folder\\sweng25_group11-microsoftspamdetection\\sklearn-env\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 9 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model, tokenizer, and threshold loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Define the file paths\n",
    "model_path = '../model/cnn_model.keras'\n",
    "info_path = '../model/cnn_model_info.pkl'\n",
    "\n",
    "# Load the Keras model\n",
    "cnn_model = load_model(model_path)\n",
    "\n",
    "# Load tokenizer and threshold\n",
    "model_info = joblib.load(info_path)\n",
    "tokenizer = model_info[\"tokenizer\"]\n",
    "threshold = model_info[\"threshold\"]\n",
    "\n",
    "print(\"CNN model, tokenizer, and threshold loaded successfully.\")\n",
    "\n",
    "from backend.TextPreprocessingUtils import preprocess_for_cnn\n",
    "\n",
    "X_test = preprocess_for_cnn(df[\"message\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "Accuracy: 0.6824\n",
      "Precision: 0.3109\n",
      "Recall: 0.7291\n",
      "F1 Score: 0.4359\n",
      "\n",
      "CNN:\n",
      "Accuracy: 0.6922\n",
      "Precision: 0.3183\n",
      "Recall: 0.7250\n",
      "F1 Score: 0.4423\n",
      "\n",
      "NB:\n",
      "Accuracy: 0.8359\n",
      "Precision: 0.6892\n",
      "Recall: 0.0458\n",
      "F1 Score: 0.0859\n",
      "CNN:\n",
      "Accuracy: 0.8316\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrjoe\\OneDrive\\Desktop\\New folder\\sweng25_group11-microsoftspamdetection\\sklearn-env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "svm_accuracy = accuracy_score(ytest, y_pred_svm)\n",
    "svm_precision = precision_score(ytest, y_pred_svm)\n",
    "svm_recall = recall_score(ytest, y_pred_svm)\n",
    "svm_f1 = f1_score(ytest, y_pred_svm)\n",
    "\n",
    "cnn_accuracy = accuracy_score(ytest, y_pred_cnn_labels)\n",
    "cnn_precision = precision_score(ytest, y_pred_cnn_labels)\n",
    "cnn_recall = recall_score(ytest, y_pred_cnn_labels)\n",
    "cnn_f1 = f1_score(ytest, y_pred_cnn_labels)\n",
    "\n",
    "good_cnn_accuracy = accuracy_score(ytest, y_pred_good_cnn)\n",
    "good_cnn_precision = precision_score(ytest, y_pred_good_cnn)\n",
    "good_cnn_recall = recall_score(ytest, y_pred_good_cnn)\n",
    "good_cnn_f1 = f1_score(ytest, y_pred_good_cnn)\n",
    "\n",
    "nb_accuracy = accuracy_score(ytest, y_pred_nb)\n",
    "nb_precision = precision_score(ytest, y_pred_nb)\n",
    "nb_recall = recall_score(ytest, y_pred_nb)\n",
    "nb_f1 = f1_score(ytest, y_pred_nb)\n",
    "\n",
    "# Display Results\n",
    "print(\"SVM:\")\n",
    "print(f\"Accuracy: {svm_accuracy:.4f}\")\n",
    "print(f\"Precision: {svm_precision:.4f}\")\n",
    "print(f\"Recall: {svm_recall:.4f}\")\n",
    "print(f\"F1 Score: {svm_f1:.4f}\")\n",
    "print(\"\")\n",
    "print(\"CNN:\")\n",
    "print(f\"Accuracy: {cnn_accuracy:.4f}\")\n",
    "print(f\"Precision: {cnn_precision:.4f}\")\n",
    "print(f\"Recall: {cnn_recall:.4f}\")\n",
    "print(f\"F1 Score: {cnn_f1:.4f}\")\n",
    "print(\"\")\n",
    "print(\"NB:\")\n",
    "print(f\"Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Precision: {nb_precision:.4f}\")\n",
    "print(f\"Recall: {nb_recall:.4f}\")\n",
    "print(f\"F1 Score: {nb_f1:.4f}\")\n",
    "print(\"CNN:\")\n",
    "print(f\"Accuracy: {good_cnn_accuracy:.4f}\")\n",
    "print(f\"Precision: {good_cnn_precision:.4f}\")\n",
    "print(f\"Recall: {good_cnn_recall:.4f}\")\n",
    "print(f\"F1 Score: {good_cnn_f1:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Logistic Regression:\n",
      "Accuracy: 0.6822\n",
      "Precision: 0.3108\n",
      "Recall: 0.7290\n",
      "F1 Score: 0.4358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "logreg.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_log = logreg.predict(xtest)\n",
    "\n",
    "# Evaluate\n",
    "log_accuracy = accuracy_score(ytest, y_pred_log)\n",
    "log_precision = precision_score(ytest, y_pred_log)\n",
    "log_recall = recall_score(ytest, y_pred_log)\n",
    "log_f1 = f1_score(ytest, y_pred_log)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {log_accuracy:.4f}\")\n",
    "print(f\"Precision: {log_precision:.4f}\")\n",
    "print(f\"Recall: {log_recall:.4f}\")\n",
    "print(f\"F1 Score: {log_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
